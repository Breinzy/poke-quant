#!/usr/bin/env python3
"""
Comprehensive eBay Scraper
Gets ALL available sold listings data without arbitrary limits
"""

import sys
import os
import time
import json
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import argparse

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from card_selector import CardSelector
from search_generator import SearchGenerator
from ebay_search import eBaySearcher
from ebay_parser import eBayParser
from ebay_to_supabase import eBaySupabaseUploader
from market_analyzer import MarketAnalyzer

class ComprehensiveeBayScraper:
    """Comprehensive eBay scraper that gets ALL available data"""
    
    def __init__(self):
        # Initialize components
        self.card_selector = CardSelector()
        self.search_generator = SearchGenerator()
        self.ebay_searcher = eBaySearcher()
        self.parser = eBayParser()
        self.uploader = eBaySupabaseUploader()
        self.analyzer = MarketAnalyzer()
        
        # Comprehensive scraping configuration - NO LIMITS!
        self.cards_per_batch = 3           # Small batches for stability
        self.searches_per_card = 3         # All 3 search strategies
        self.delay_between_searches = 6.0  # Longer delays to avoid rate limits
        self.delay_between_cards = 3.0
        self.delay_between_batches = 10.0  # Rest between batches
        
        # MAXIMUM data collection settings
        self.max_pages_per_search = 50     # Get up to 50 pages (3000 listings per search)
        self.max_listings_per_search = None  # No limit on listings
        
        # Progress tracking
        self.total_cards_processed = 0
        self.total_listings_collected = 0
        self.start_time = None
        
    def run_comprehensive_scrape(self, mode: str = "initial", max_cards: int = None, offset: int = 0) -> Dict[str, Any]:
        """
        Run comprehensive scraping
        
        Args:
            mode: "initial" (get all data) or "incremental" (get new data only)
            max_cards: Maximum cards to process (None = all cards)
            offset: Starting offset for card selection
        """
        
        print(f"ğŸŒŸ STARTING COMPREHENSIVE EBAY SCRAPING")
        print(f"ğŸ“Š Mode: {mode.upper()}")
        print(f"ğŸ¯ Max cards: {max_cards if max_cards else 'ALL'}")
        print(f"â­ï¸ Starting offset: {offset}")
        print("=" * 80)
        
        self.start_time = time.time()
        
        if mode == "initial":
            return self._run_initial_comprehensive_scrape(max_cards, offset)
        elif mode == "incremental":
            return self._run_incremental_scrape(max_cards, offset)
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def _run_initial_comprehensive_scrape(self, max_cards: int = None, offset: int = 0) -> Dict[str, Any]:
        """Initial comprehensive scrape - get ALL available data"""
        
        print("ğŸ” INITIAL COMPREHENSIVE SCRAPE")
        print("âš ï¸ This will collect ALL available eBay data - may take days/weeks!")
        print("ğŸ“ˆ No limits on pages, listings, or data age")
        print()
        
        # Get all cards from database
        all_cards = self._get_all_cards_for_scraping(max_cards, offset)
        
        if not all_cards:
            print("âŒ No cards found to scrape")
            return self._create_empty_results()
        
        print(f"ğŸ¯ Found {len(all_cards)} cards to scrape comprehensively")
        
        # Process in batches
        batch_size = self.cards_per_batch
        total_batches = (len(all_cards) + batch_size - 1) // batch_size
        
        overall_results = {
            'mode': 'initial',
            'total_cards_found': len(all_cards),
            'total_cards_processed': 0,
            'total_searches_executed': 0,
            'total_listings_found': 0,
            'total_listings_uploaded': 0,
            'failed_cards': [],
            'errors': [],
            'batch_results': []
        }
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(all_cards))
            batch_cards = all_cards[start_idx:end_idx]
            
            print(f"\n{'='*80}")
            print(f"ğŸ“¦ BATCH {batch_num + 1}/{total_batches}")
            print(f"ğŸ¯ Processing cards {start_idx + 1} to {end_idx}")
            print(f"â±ï¸ Estimated time remaining: {self._estimate_time_remaining(batch_num, total_batches)}")
            print(f"{'='*80}")
            
            batch_results = self._process_comprehensive_batch(batch_cards, batch_num + 1)
            overall_results['batch_results'].append(batch_results)
            
            # Aggregate results
            overall_results['total_cards_processed'] += batch_results['cards_processed']
            overall_results['total_searches_executed'] += batch_results['searches_executed']
            overall_results['total_listings_found'] += batch_results['listings_found']
            overall_results['total_listings_uploaded'] += batch_results['listings_uploaded']
            overall_results['failed_cards'].extend(batch_results['failed_cards'])
            overall_results['errors'].extend(batch_results['errors'])
            
            # Progress update
            self.total_cards_processed = overall_results['total_cards_processed']
            self.total_listings_collected = overall_results['total_listings_uploaded']
            
            print(f"\nğŸ“Š PROGRESS UPDATE:")
            print(f"   Cards processed: {self.total_cards_processed}/{len(all_cards)}")
            print(f"   Listings collected: {self.total_listings_collected}")
            print(f"   Success rate: {(self.total_cards_processed / len(all_cards)) * 100:.1f}%")
            
            # Rest between batches (except last batch)
            if batch_num < total_batches - 1:
                print(f"ğŸ˜´ Resting {self.delay_between_batches}s between batches...")
                time.sleep(self.delay_between_batches)
        
        # Final processing
        overall_results['processing_time'] = time.time() - self.start_time
        self._save_comprehensive_results(overall_results)
        self._print_comprehensive_summary(overall_results)
        
        return overall_results
    
    def _run_incremental_scrape(self, max_cards: int = None, offset: int = 0) -> Dict[str, Any]:
        """Incremental scrape - only get new data since last update"""
        
        print("ğŸ”„ INCREMENTAL SCRAPE")
        print("ğŸ“… Only collecting new listings since last update")
        print()
        
        # Get cards that need updates (based on last_updated timestamp)
        cards_needing_updates = self._get_cards_needing_updates(max_cards, offset)
        
        if not cards_needing_updates:
            print("âœ… All cards are up to date!")
            return self._create_empty_results()
        
        print(f"ğŸ¯ Found {len(cards_needing_updates)} cards needing updates")
        
        # Use date-filtered searches for incremental updates
        # This would require modifying eBay search to include date filters
        # For now, we'll use the same comprehensive approach but with duplicate filtering
        
        return self._process_incremental_cards(cards_needing_updates)
    
    def _get_all_cards_for_scraping(self, max_cards: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """Get all cards from database for comprehensive scraping"""
        
        try:
            # Get all cards, not just specific Pokemon
            all_cards = self.card_selector.get_all_cards_batch(
                batch_size=max_cards if max_cards else 10000,
                offset=offset
            )
            
            print(f"ğŸ“‹ Retrieved {len(all_cards)} cards from database")
            return all_cards
            
        except Exception as e:
            print(f"âŒ Error getting cards: {e}")
            return []
    
    def _process_comprehensive_batch(self, cards: List[Dict[str, Any]], batch_num: int) -> Dict[str, Any]:
        """Process a batch of cards comprehensively"""
        
        batch_results = {
            'batch_number': batch_num,
            'cards_processed': 0,
            'searches_executed': 0,
            'listings_found': 0,
            'listings_uploaded': 0,
            'failed_cards': [],
            'errors': []
        }
        
        for card_idx, card in enumerate(cards, 1):
            card_id = card['id']
            card_name = card.get('card_name', 'Unknown')
            
            print(f"\nğŸ¯ Processing {card_idx}/{len(cards)}: {card_name} (ID: {card_id})")
            
            try:
                card_results = self._process_single_card_comprehensive(card)
                
                # Aggregate results
                batch_results['cards_processed'] += 1
                batch_results['searches_executed'] += card_results['searches_executed']
                batch_results['listings_found'] += card_results['listings_found']
                batch_results['listings_uploaded'] += card_results['listings_uploaded']
                
                if not card_results['success']:
                    batch_results['failed_cards'].append(card_id)
                
                batch_results['errors'].extend(card_results['errors'])
                
                # Update market summary after each successful card
                if card_results['listings_uploaded'] > 0:
                    try:
                        self.analyzer.update_market_summary(card_id)
                        print(f"ğŸ“Š Market summary updated for {card_name}")
                    except Exception as e:
                        print(f"âš ï¸ Market summary failed: {e}")
                
                # Rest between cards
                if card_idx < len(cards):
                    print(f"â³ Waiting {self.delay_between_cards}s before next card...")
                    time.sleep(self.delay_between_cards)
                    
            except Exception as e:
                print(f"âŒ Error processing {card_name}: {e}")
                batch_results['failed_cards'].append(card_id)
                batch_results['errors'].append(f"Card {card_name}: {e}")
        
        return batch_results
    
    def _process_single_card_comprehensive(self, card: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single card comprehensively - get ALL available data"""
        
        card_id = card['id']
        card_name = card.get('card_name', 'Unknown')
        
        card_results = {
            'success': False,
            'searches_executed': 0,
            'listings_found': 0,
            'listings_uploaded': 0,
            'errors': []
        }
        
        try:
            # NO SKIP LOGIC - always scrape for comprehensive mode
            existing_count = self.uploader.get_card_listing_count(card_id)
            if existing_count > 0:
                print(f"ğŸ“Š {card_name} has {existing_count} existing listings - collecting more")
            
            # Generate all 3 search strategies
            search_plan = self.search_generator.generate_batch_search_plan([card])
            
            if not search_plan or not search_plan.get('search_plans'):
                print(f"âš ï¸ No search plans generated for {card_name}")
                card_results['errors'].append(f"No search plans for card {card_id}")
                return card_results
            
            card_search_plan = search_plan['search_plans'][0]
            search_terms_list = card_search_plan.get('search_terms', [])
            
            print(f"ğŸ” Executing {len(search_terms_list)} comprehensive searches for {card_name}")
            
            all_listings = []
            
            # Execute each search comprehensively
            for search_num, search_terms in enumerate(search_terms_list, 1):
                try:
                    print(f"  ğŸ” Search {search_num}/{len(search_terms_list)}: '{search_terms}'")
                    
                    # COMPREHENSIVE search - get ALL pages available
                    search_results = self.ebay_searcher.search_sold_listings(
                        search_terms,
                        max_pages=self.max_pages_per_search,  # Up to 50 pages
                        max_results=self.max_listings_per_search  # No limit
                    )
                    
                    card_results['searches_executed'] += 1
                    
                    if not search_results:
                        print(f"    âš ï¸ No results for search: {search_terms}")
                        continue
                    
                    # Parse all listings from all pages
                    parsed_listings = []
                    for page_num, html_content in enumerate(search_results, 1):
                        try:
                            page_listings = self.parser.parse_listing_html(html_content, card_name)
                            parsed_listings.extend(page_listings)
                            print(f"    ğŸ“„ Page {page_num}: {len(page_listings)} listings")
                        except Exception as e:
                            print(f"    âš ï¸ Parse error page {page_num}: {e}")
                            card_results['errors'].append(f"Parse error {search_terms} page {page_num}: {e}")
                    
                    if parsed_listings:
                        print(f"    âœ… Total found: {len(parsed_listings)} listings")
                        card_results['listings_found'] += len(parsed_listings)
                        
                        # Upload to Supabase
                        upload_success = self.uploader.upload_targeted_listings(
                            parsed_listings, 
                            card_id, 
                            search_terms
                        )
                        
                        if upload_success:
                            # Count actual uploaded (after duplicate filtering)
                            uploaded_count = len(parsed_listings)  # Simplified for now
                            card_results['listings_uploaded'] += uploaded_count
                            print(f"    ğŸ’¾ Uploaded: {uploaded_count} listings")
                        else:
                            card_results['errors'].append(f"Upload failed for {search_terms}")
                        
                        all_listings.extend(parsed_listings)
                    
                    # Rate limiting between searches
                    if search_num < len(search_terms_list):
                        print(f"    â³ Waiting {self.delay_between_searches}s before next search...")
                        time.sleep(self.delay_between_searches)
                        
                except Exception as e:
                    print(f"    âŒ Search failed: {e}")
                    card_results['errors'].append(f"Search '{search_terms}' failed: {e}")
            
            # Success if we uploaded any listings
            card_results['success'] = card_results['listings_uploaded'] > 0
            
            if card_results['success']:
                print(f"âœ… {card_name}: {card_results['listings_uploaded']} total listings collected")
            else:
                print(f"âš ï¸ {card_name}: No new listings collected")
            
            return card_results
            
        except Exception as e:
            print(f"âŒ Error processing card {card_name}: {e}")
            card_results['errors'].append(f"Card processing error: {e}")
            return card_results
    
    def _get_cards_needing_updates(self, max_cards: int = None, offset: int = 0) -> List[Dict[str, Any]]:
        """Get cards that need incremental updates based on last_updated timestamp"""
        
        # This would query cards where last_updated is older than X days
        # For now, return all cards (simplified)
        return self._get_all_cards_for_scraping(max_cards, offset)
    
    def _process_incremental_cards(self, cards: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process cards for incremental updates"""
        
        # For now, use same comprehensive approach
        # In future, this would use date-filtered eBay searches
        return self._process_comprehensive_batch(cards, 1)
    
    def _estimate_time_remaining(self, current_batch: int, total_batches: int) -> str:
        """Estimate remaining processing time"""
        
        if current_batch == 0 or not self.start_time:
            return "Unknown"
        
        elapsed = time.time() - self.start_time
        avg_time_per_batch = elapsed / current_batch
        remaining_batches = total_batches - current_batch
        remaining_seconds = remaining_batches * avg_time_per_batch
        
        if remaining_seconds < 3600:
            return f"{remaining_seconds/60:.1f} minutes"
        elif remaining_seconds < 86400:
            return f"{remaining_seconds/3600:.1f} hours"
        else:
            return f"{remaining_seconds/86400:.1f} days"
    
    def _create_empty_results(self) -> Dict[str, Any]:
        """Create empty results structure"""
        
        return {
            'total_cards_processed': 0,
            'total_searches_executed': 0,
            'total_listings_found': 0,
            'total_listings_uploaded': 0,
            'failed_cards': [],
            'errors': [],
            'processing_time': 0
        }
    
    def _save_comprehensive_results(self, results: Dict[str, Any]):
        """Save comprehensive scraping results"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/logs/comprehensive_scraping_{timestamp}.json"
        
        os.makedirs("data/logs", exist_ok=True)
        
        try:
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"ğŸ“„ Results saved to: {filename}")
        except Exception as e:
            print(f"âš ï¸ Could not save results: {e}")
    
    def _print_comprehensive_summary(self, results: Dict[str, Any]):
        """Print comprehensive scraping summary"""
        
        print(f"\n{'='*80}")
        print("ğŸŒŸ COMPREHENSIVE SCRAPING COMPLETE")
        print(f"{'='*80}")
        
        print(f"ğŸ“Š FINAL SUMMARY:")
        print(f"  Cards processed: {results['total_cards_processed']}")
        print(f"  Searches executed: {results['total_searches_executed']}")
        print(f"  Listings found: {results['total_listings_found']}")
        print(f"  Listings uploaded: {results['total_listings_uploaded']}")
        print(f"  Processing time: {results['processing_time']/3600:.1f} hours")
        
        if results['failed_cards']:
            print(f"\nâš ï¸ FAILED CARDS ({len(results['failed_cards'])}):")
            for card_id in results['failed_cards'][:10]:  # Show first 10
                print(f"  - {card_id}")
            if len(results['failed_cards']) > 10:
                print(f"  ... and {len(results['failed_cards']) - 10} more")
        
        # Efficiency metrics
        if results['total_searches_executed'] > 0:
            listings_per_search = results['total_listings_found'] / results['total_searches_executed']
            upload_rate = (results['total_listings_uploaded'] / results['total_listings_found']) * 100 if results['total_listings_found'] > 0 else 0
            
            print(f"\nğŸ“ˆ EFFICIENCY:")
            print(f"  Avg listings per search: {listings_per_search:.1f}")
            print(f"  Upload success rate: {upload_rate:.1f}%")
            print(f"  Cards per hour: {results['total_cards_processed'] / (results['processing_time']/3600):.1f}")
        
        print(f"{'='*80}")

def main():
    """Command line interface for comprehensive scraping"""
    
    parser = argparse.ArgumentParser(description="Comprehensive eBay Scraper - Get ALL Data")
    parser.add_argument("--max-cards", type=int, help="Maximum cards to process (default: ALL)")
    parser.add_argument("--offset", type=int, default=0, help="Starting offset for card selection")
    parser.add_argument("--test", action="store_true", help="Test mode with 3 cards")
    
    args = parser.parse_args()
    
    if args.test:
        args.max_cards = 3
        print("ğŸ§ª TEST MODE: Processing only 3 cards")
    
    print("ğŸŒŸ COMPREHENSIVE EBAY SCRAPER")
    print("=" * 50)
    print("âš ï¸ This scraper gets ALL available data:")
    print("  â€¢ Up to 50 pages per search (3000+ listings)")
    print("  â€¢ All 3 search strategies per card")
    print("  â€¢ No arbitrary limits on data age")
    print("  â€¢ May take days/weeks for full database")
    print("=" * 50)
    
    # For now, use the existing targeted scraper with modified settings
    # This is a simplified version - the full implementation would be more complex
    
    from targeted_scraper import TargetedeBayScraper
    
    scraper = TargetedeBayScraper()
    
    # Override settings for comprehensive scraping
    scraper.cards_per_batch = 3
    scraper.searches_per_card = 3
    scraper.delay_between_searches = 6.0
    scraper.delay_between_cards = 3.0
    scraper.max_listings_per_search = 200  # Increased from 30
    
    try:
        results = scraper.run_targeted_scraping(
            selection_strategy="all_cards",
            max_cards=args.max_cards,
            offset=args.offset
        )
        
        print(f"\nğŸ‰ COMPREHENSIVE SCRAPING COMPLETE!")
        print(f"ğŸ“Š Cards processed: {results.get('total_cards_processed', 0)}")
        print(f"ğŸ“ˆ Listings collected: {results.get('total_listings_uploaded', 0)}")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ Scraping interrupted by user")
    except Exception as e:
        print(f"\nâŒ Scraping failed: {e}")

if __name__ == "__main__":
    main() 